# -*- coding: utf-8 -*-
"""Text-Detoxification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SzBSDNIazZSSbXOi12KEaKI6PQlSjJMW
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !wget https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip
# !unzip filtered_paranmt.zip

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers[torch] datasets evaluate sacrebleu
# !pip install accelerate -U
# !pip install wandb

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer, DataCollatorForSeq2Seq
import evaluate

import numpy as np
from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
import os
from transformers import pipeline

df = pd.read_csv('filtered.tsv', sep='\t').drop(columns=['Unnamed: 0'])
df

(df.ref_tox < df.trn_tox).mean()

cp_reference = df.reference.copy()
cp_tox_reference = df.ref_tox.copy()
mask = df.ref_tox < df.trn_tox
df.loc[mask, 'reference'] = df.loc[mask, 'translation']
df.loc[mask, 'translation'] = cp_reference

df.loc[mask, 'ref_tox'] = df.loc[mask, 'trn_tox']
df.loc[mask, 'trn_tox'] = cp_tox_reference

(df.ref_tox < df.trn_tox).mean()

df

"""Split Dataset into train and test subsets"""

hf_df = Dataset.from_pandas(df)
hf_df = hf_df.train_test_split(test_size=0.2)

hf_df

"""Translate the data into an appropriate format for T5 fine-tuning."""

checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

prefix = "Perform Text-Detoxification: "


def preprocess_function(examples):
    inputs = [prefix + example for example in examples['reference']]
    targets = [example for example in examples['translation']]
    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
    return model_inputs

hf_df = hf_df.map(preprocess_function, batched=True)

"""Create Collator object for dynamic text padding"""

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)

"""Metric Evaluation Code"""

metric = evaluate.load("sacrebleu")


def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {"bleu": result["score"]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result

"""Fine-Tune the model"""

!wandb login

!huggingface-cli login

os.environ["WANDB_PROJECT"] = "text_detoxification"
os.environ["WANDB_GROUP"] = "T5_seq2seq"
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

training_args = Seq2SeqTrainingArguments(
    output_dir="output",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=2,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
    report_to="wandb"
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=hf_df["train"],
    eval_dataset=hf_df["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.push_to_hub()

!huggingface-cli login

detoxifier = pipeline("translation", model="ummagumm-a/output", revision='44a0213')

"""Examples of toxic texts where the model shines in its cleaning abilities."""

for i in [5, 10, 12, 13, 14, 15]:
    print('-'*100)
    print("REFERENCE:", hf_df['test'][i]['reference'])
    print("TRANSLATION:", hf_df['test'][i]['translation'])
    print("MODEL TRANSLATION:", detoxifier(hf_df['test'][i]['reference']))

